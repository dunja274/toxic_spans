{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import FastText\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "torch.manual_seed(2021)\n",
    "\n",
    "import podium\n",
    "from podium import BucketIterator\n",
    "from podium.datasets import TabularDataset\n",
    "from podium import Vocab\n",
    "from podium.storage import Field\n",
    "from podium.storage import LabelField\n",
    "from podium.storage.vectorizers import GloVe\n",
    "\n",
    "import toxic_util as TU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicijalizacija tokenizatora i lematizatora\n",
    "tokenizer = podium.preproc.get_tokenizer('spacy')\n",
    "lemmatizer = podium.preproc.SpacyLemmatizer()    \n",
    "\n",
    "# inicijalizacija praznog vokabulara\n",
    "vocab = Vocab()\n",
    "\n",
    "# definicija polja, odnosno stupaca dataseta\n",
    "fields = {\n",
    "    'text': Field(\n",
    "        'text', \n",
    "        pretokenize_hooks=[str.lower],   # prije tokeniziranja baci sve u lowercase\n",
    "        tokenizer=tokenizer,             # tokeniziraj s definiranim tokenizatorom\n",
    "        posttokenize_hooks=[lemmatizer], # nakon tokenizacije uzmi samo lemme riječi \n",
    "        numericalizer=vocab,             # veza dataseta i vokabulara, tj. embedding matrice\n",
    "        keep_raw=True                    # debug svrhe\n",
    "    ),\n",
    "    'spans': Field(\n",
    "        'spans', \n",
    "        tokenizer=ast.literal_eval,      # jer se čita iz csva, eval obavlja evaluaciju stringa kao da je kod\n",
    "        disable_batch_matrix=True\n",
    "    ),\n",
    "    'labels': Field(\n",
    "        'labels',\n",
    "        tokenizer=ast.literal_eval,      # isto kao i iznad\n",
    "        is_target=True,\n",
    "        numericalizer=np.array,          # treba za padding\n",
    "        padding_token=-100\n",
    "    )\n",
    "}\n",
    "\n",
    "# učitaj dataset\n",
    "original = TabularDataset(\n",
    "    path = 'data/tsd_train_with_labels.csv', # prepravljen dataset s labelama već instaliranim\n",
    "    format = 'csv', \n",
    "    fields = fields\n",
    ")\n",
    "\n",
    "# split na set za treniranje i set za validaciju\n",
    "train, valid = original.split(0.8)\n",
    "\n",
    "# inicijaliziraj embedding matricu\n",
    "glove = GloVe()\n",
    "embeddings = glove.load_vocab(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in train:\n",
    "    assert len(t.text[1]) == len(t.labels[1]), f'pad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ako je ovo True, ispisuju se DEBUG izjave i model napravi samo jednu epohu s jednim retkom\n",
    "dbg = False\n",
    "def DEBUG(fstring):\n",
    "    if(dbg): print(fstring)\n",
    "        \n",
    "vbs = False\n",
    "def VERBOSE(fstring):\n",
    "    if(vbs): print(fstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_lens(batch):\n",
    "    ret = []\n",
    "    batch = batch.tolist()\n",
    "    for seq in batch:\n",
    "        seq_len = len(seq)\n",
    "        for x in seq:\n",
    "            if x == 1:\n",
    "                seq_len -= 1\n",
    "        ret.append(seq_len)\n",
    "    return ret\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, word_embeds, vocab, hidden_dim):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = word_embeds.shape[0]\n",
    "        self.embeddings = word_embeds\n",
    "        self.embed_dim = word_embeds.shape[1]\n",
    "        self.lstm1 = nn.LSTM(word_embeds.shape[1], hidden_dim*2, batch_first = True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim*2, hidden_dim, batch_first = True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, 3)\n",
    "\n",
    "    def forward(self, text_batch):\n",
    "        DEBUG('######################### FORWARD ############################')\n",
    "        DEBUG(f'text_batch\\n{text_batch}')\n",
    "        \n",
    "        input_lens = seq_lens(text_batch)\n",
    "        \n",
    "        DEBUG(f'input_lens\\n{input_lens}')\n",
    "        \n",
    "        sent_vec = torch.from_numpy(embeddings[text_batch])\n",
    "        sent_vec.requires_grad = False\n",
    "        sent_len = len(text_batch[0])\n",
    "        \n",
    "        DEBUG(f'sent_vec\\n{sent_vec.shape}\\n{sent_vec}')\n",
    "        \n",
    "        packed_sent_vec = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            sent_vec.view(-1, sent_len, self.embed_dim),\n",
    "            input_lens,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        ) \n",
    "        \n",
    "        # ova var se ne koristi, sluzi samo za debug purposes\n",
    "        unpacked_sent_vec, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_sent_vec, batch_first=True)\n",
    "        DEBUG(f'unpacked_sent_vec\\n{unpacked_sent_vec.shape}\\n{unpacked_sent_vec}')\n",
    "        \n",
    "        DEBUG(f'packed_sent_vec\\n{packed_sent_vec}')\n",
    "        \n",
    "        packed_lstm_out, _ = self.lstm1(packed_sent_vec.float())\n",
    "        packed_lstm_out, _ = self.lstm2(packed_lstm_out)\n",
    "        \n",
    "        DEBUG(f'packed_lstm_out\\n{packed_lstm_out}')\n",
    "        \n",
    "        lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_lstm_out, batch_first=True)\n",
    "    \n",
    "        DEBUG(f'lstm_out\\n{lstm_out.shape}\\n{lstm_out}')\n",
    "        \n",
    "        tag_space = self.hidden2tag(lstm_out.view(-1, sent_len, self.hidden_dim))\n",
    "        \n",
    "        DEBUG(f'tag_space\\n{tag_space.shape}\\n{tag_space}')\n",
    "        \n",
    "        DEBUG('######################### END FORWARD ########################')\n",
    "        return tag_space\n",
    "    \n",
    "    def predict(self, numericalized_sent):\n",
    "        with torch.no_grad():\n",
    "            tag_scores = self(numericalized_sent)\n",
    "            labels = TU.labels_from_tensor(tag_scores.view(-1, 3))\n",
    "            return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_length(row):\n",
    "    # row ... instanca objekta Example, i.e. redak u datasetu\n",
    "    _, tokenized = row.text\n",
    "    # row.text u sebi sadrži i raw i tokenized podatke, prvi se odbacuje\n",
    "    return len(tokenized)\n",
    "\n",
    "def fit(BSIZE, LR, NEPOCHS, embeddings, vocab):    \n",
    "    trainit = BucketIterator(train, batch_size=BSIZE, bucket_sort_key=instance_length)\n",
    "    model = LSTMTagger(embeddings, vocab, 300)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    print(f'BSIZE {BSIZE} LR {LR} NEPOCHS {NEPOCHS}')\n",
    "    \n",
    "    for epoch in range(NEPOCHS):\n",
    "        for input_batch, target_batch in trainit:\n",
    "            if input_batch.text.shape[0] != BSIZE: continue\n",
    "                \n",
    "            DEBUG('############## FIT ###############')\n",
    "            \n",
    "            text_batch = input_batch.text\n",
    "            label_batch = torch.tensor(target_batch.labels)\n",
    "            text_batch = torch.tensor(text_batch)\n",
    "            \n",
    "            DEBUG(f'text_batch\\n{text_batch}')\n",
    "            DEBUG(f'label_batch\\n{label_batch}')\n",
    "            \n",
    "            model.zero_grad()            \n",
    "            label_scores = model(text_batch)\n",
    "            \n",
    "            DEBUG(f'label_scores\\n{label_scores.shape}\\n{label_scores}')\n",
    "            DEBUG(f'label_batch\\n{label_batch.shape}\\n{label_batch}')\n",
    "            # radi dobro\n",
    "            loss = loss_function(label_scores.view(BSIZE, 3, -1), label_batch.view(BSIZE, -1).long())\n",
    "            \n",
    "            DEBUG(f'loss\\n{type(loss)}\\n{loss}\\n{loss.grad_fn}')\n",
    "            VERBOSE(f'loss {loss}')\n",
    "            loss.backward()\n",
    "        \n",
    "            optimizer.step()\n",
    "            if(dbg): return None\n",
    "     \n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "    score = 0\n",
    "    failed = 0\n",
    "    succeeded = 0\n",
    "    for row in valid:\n",
    "        spans = row.spans[1]\n",
    "        text = row.text[1]\n",
    "        labels = row.labels[1]\n",
    "        numericalized = [vocab.stoi[word] for word in text]\n",
    "        numericalized = torch.tensor(numericalized)\n",
    "\n",
    "        try:\n",
    "            predicted_labels = model.predict(numericalized.view(1, -1))\n",
    "            predicted_spans = TU.spans_from_labels(row.text[0], predicted_labels, nlp)\n",
    "            score += TU.f1(predicted_spans, spans)\n",
    "            succeeded += 1\n",
    "        except: \n",
    "            failed += 1 # za svaki slucaj\n",
    "            \n",
    "    score = float(score) / succeeded\n",
    "    print(f'F1 {score}')\n",
    "    if(failed != 0): print(f'FAILED {failed}')\n",
    "\n",
    "    return model, score, f'BSIZE {BSIZE} LR {LR} NEPOCHS {NEPOCHS}'\n",
    "\n",
    "if dbg: fit(32, 0.001, 1, embeddings, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ovako to izgleda interno za jedan batch s 32 inputa, vidimo da su 0 dobro postavljene\n",
    "# loss tensor([[1.0987, 0.0000, 0.0000],\n",
    "#         [1.1112, 0.0000, 0.0000],\n",
    "#         [1.0995, 0.0000, 0.0000],\n",
    "#         [1.0841, 0.0000, 0.0000],\n",
    "#         [1.1162, 0.0000, 0.0000],\n",
    "#         [1.0818, 0.0000, 0.0000],\n",
    "#         [1.1162, 0.0000, 0.0000],\n",
    "#         [1.1371, 0.0000, 0.0000],\n",
    "#         [1.1340, 0.0000, 0.0000],\n",
    "#         [1.0991, 0.0000, 0.0000],\n",
    "#         [1.1141, 0.0000, 0.0000],\n",
    "#         [1.0872, 1.1260, 0.0000],\n",
    "#         [1.0942, 1.0652, 0.0000],\n",
    "#         [0.9729, 1.1223, 0.0000],\n",
    "#         [1.1731, 1.1258, 0.0000],\n",
    "#         [1.0989, 1.1069, 0.0000],\n",
    "#         [1.0427, 1.0527, 0.0000],\n",
    "#         [0.9409, 1.1290, 0.0000],\n",
    "#         [0.9859, 1.1236, 0.0000],\n",
    "#         [1.0108, 1.0835, 0.0000],\n",
    "#         [1.1639, 1.1037, 0.0000],\n",
    "#         [1.0723, 1.0049, 0.0000],\n",
    "#         [0.9820, 1.0845, 0.0000],\n",
    "#         [1.1454, 1.1492, 0.0000],\n",
    "#         [1.0974, 1.1455, 1.0829],\n",
    "#         [1.0626, 1.0231, 1.0930],\n",
    "#         [1.0369, 1.1696, 1.1302],\n",
    "#         [1.1067, 1.0430, 1.1548],\n",
    "#         [1.0622, 1.0870, 1.1068],\n",
    "#         [1.1639, 1.0571, 1.0314],\n",
    "#         [1.1049, 1.1777, 1.1405],\n",
    "#         [1.1957, 1.0839, 0.9871]], grad_fn=<ViewBackward>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(batch_sizes, learning_rates, n_epochs, embeddings, vocab):\n",
    "    best_model = None\n",
    "    best_score = 0.0\n",
    "    best_desc = ''\n",
    "    for bs in batch_sizes:\n",
    "        for lr in learning_rates:\n",
    "            for n in n_epochs:\n",
    "                model, score, model_desc = fit(bs, lr, n, embeddings, vocab)\n",
    "                if score > best_score:\n",
    "                    best_model = model\n",
    "                    best_score = score\n",
    "                    best_desc = model_desc\n",
    "    return best_model, best_score, best_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BSIZE 32 LR 0.1 NEPOCHS 1\n",
      "F1 0.23429456762504963\n",
      "Best score > 0.23429456762504963\n",
      "Achieved for hyperparameters > BSIZE 32 LR 0.1 NEPOCHS 1\n"
     ]
    }
   ],
   "source": [
    "# dobavi najbolji model\n",
    "model, score, desc = grid_search([32], [0.1], [1], embeddings, vocab)\n",
    "print(f'Best score > {score}')\n",
    "print(f'Achieved for hyperparameters > {desc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[15, 16, 17, 18, 19, 27, 28, 29, 30, 31]</td>\n",
       "      <td>Because he's a moron and a bigot. It's not any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[29, 30, 31, 32, 33, 34]</td>\n",
       "      <td>How about we stop protecting idiots and let na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[166, 167, 168, 169, 170, 171]</td>\n",
       "      <td>If people  were  smart, they would  Boycott th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[87, 88, 89, 90, 91, 92]</td>\n",
       "      <td>Trump Claimed that Russia will never invade th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>As long as your willing to pay a lot more for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>[129, 130, 131, 132, 133, 134]</td>\n",
       "      <td>But ... Trump's not bluffing. He's prepared to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>[126, 127, 128, 129, 130, 131]</td>\n",
       "      <td>Can't believe the limited knowledge of this Ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>[24, 25, 26, 27, 28, 29]</td>\n",
       "      <td>I think it conservative idiots who cannot reac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>You're an id*ot...Go away.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>[136, 137, 138, 139, 140, 141]</td>\n",
       "      <td>Unless there is wording in the employment cont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 spans  \\\n",
       "0             [15, 16, 17, 18, 19, 27, 28, 29, 30, 31]   \n",
       "1                             [29, 30, 31, 32, 33, 34]   \n",
       "2                       [166, 167, 168, 169, 170, 171]   \n",
       "3                             [87, 88, 89, 90, 91, 92]   \n",
       "4                                                   []   \n",
       "..                                                 ...   \n",
       "685                     [129, 130, 131, 132, 133, 134]   \n",
       "686                     [126, 127, 128, 129, 130, 131]   \n",
       "687                           [24, 25, 26, 27, 28, 29]   \n",
       "688  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "689                     [136, 137, 138, 139, 140, 141]   \n",
       "\n",
       "                                                  text  \n",
       "0    Because he's a moron and a bigot. It's not any...  \n",
       "1    How about we stop protecting idiots and let na...  \n",
       "2    If people  were  smart, they would  Boycott th...  \n",
       "3    Trump Claimed that Russia will never invade th...  \n",
       "4    As long as your willing to pay a lot more for ...  \n",
       "..                                                 ...  \n",
       "685  But ... Trump's not bluffing. He's prepared to...  \n",
       "686  Can't believe the limited knowledge of this Ar...  \n",
       "687  I think it conservative idiots who cannot reac...  \n",
       "688                         You're an id*ot...Go away.  \n",
       "689  Unless there is wording in the employment cont...  \n",
       "\n",
       "[690 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('data/tsd_trial.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2291812963836909\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "fail=0\n",
    "for idx, row in test.iterrows():\n",
    "    try:\n",
    "        spans = ast.literal_eval(row[0])\n",
    "        text = row[1]\n",
    "\n",
    "        tokenized = tokenizer(text.lower())\n",
    "        _, lemmatized = lemmatizer(text.lower(), tokenized)\n",
    "\n",
    "        numericalized = [vocab.stoi[word] for word in lemmatized]\n",
    "\n",
    "        numericalized = torch.tensor(numericalized)\n",
    "        batch = numericalized.view(1, -1)\n",
    "\n",
    "        predicted_labels = model.predict(batch)\n",
    "\n",
    "        predicted_spans = TU.spans_from_labels(text, predicted_labels, nlp)\n",
    "\n",
    "        row_score = TU.f1(predicted_spans, spans)\n",
    "        score += row_score\n",
    "    except:\n",
    "        fail += 1\n",
    "score /= test.shape[0]\n",
    "print(score)\n",
    "print(fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
