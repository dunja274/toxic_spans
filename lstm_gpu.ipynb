{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "torch.manual_seed(2021)\n",
    "\n",
    "import podium\n",
    "from podium import BucketIterator\n",
    "from podium.datasets import TabularDataset\n",
    "from podium import Vocab\n",
    "from podium.storage import Field\n",
    "from podium.storage import LabelField\n",
    "from podium.storage.vectorizers import GloVe\n",
    "\n",
    "import toxic_util as TU\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicijalizacija tokenizatora i lematizatora\n",
    "tokenizer = podium.preproc.get_tokenizer('spacy')\n",
    "lemmatizer = podium.preproc.SpacyLemmatizer()    \n",
    "\n",
    "# inicijalizacija praznog vokabulara\n",
    "vocab = Vocab()\n",
    "\n",
    "# definicija polja, odnosno stupaca dataseta\n",
    "fields = {\n",
    "    'text': Field(\n",
    "        'text', \n",
    "        pretokenize_hooks=[str.lower],   # prije tokeniziranja baci sve u lowercase\n",
    "        tokenizer=tokenizer,             # tokeniziraj s definiranim tokenizatorom\n",
    "        posttokenize_hooks=[lemmatizer], # nakon tokenizacije uzmi samo lemme riječi \n",
    "        numericalizer=vocab,             # veza dataseta i vokabulara, tj. embedding matrice\n",
    "        keep_raw=True                    # debug svrhe\n",
    "    ),\n",
    "    'spans': Field(\n",
    "        'spans', \n",
    "        tokenizer=ast.literal_eval,      # jer se čita iz csva, eval obavlja evaluaciju stringa kao da je kod\n",
    "        disable_batch_matrix=True\n",
    "    ),\n",
    "    'labels': Field(\n",
    "        'labels',\n",
    "        tokenizer=ast.literal_eval,      # isto kao i iznad\n",
    "        is_target=True,\n",
    "        numericalizer=np.array,          # treba za padding\n",
    "        padding_token=-100\n",
    "    )\n",
    "}\n",
    "\n",
    "# učitaj dataset\n",
    "original = TabularDataset(\n",
    "    path = 'data/tsd_train_with_labels.csv', # prepravljen dataset s labelama već instaliranim\n",
    "    format = 'csv', \n",
    "    fields = fields\n",
    ")\n",
    "\n",
    "# split na set za treniranje i set za validaciju\n",
    "train, valid = original.split(0.8)\n",
    "\n",
    "# inicijaliziraj embedding matricu\n",
    "glove = GloVe()\n",
    "embeddings = glove.load_vocab(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for t in train:\n",
    "    assert len(t.text[1]) == len(t.labels[1]), f'pad'\n",
    "    \n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ako je ovo True, ispisuju se DEBUG izjave i model napravi samo jednu epohu s jednim retkom\n",
    "dbg = False\n",
    "def DEBUG(fstring):\n",
    "    if(dbg): print(fstring)\n",
    "        \n",
    "vbs = False\n",
    "def VERBOSE(fstring):\n",
    "    if(vbs): print(fstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_lens(batch):\n",
    "    ret = []\n",
    "    batch = batch.tolist()\n",
    "    for seq in batch:\n",
    "        seq_len = len(seq)\n",
    "        for x in seq:\n",
    "            if x == 1:\n",
    "                seq_len -= 1\n",
    "        ret.append(seq_len)\n",
    "    return ret\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, base_model, word_embeds, vocab, hidden_dim):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = word_embeds.shape[0]\n",
    "        self.embeddings = word_embeds\n",
    "        self.embed_dim = word_embeds.shape[1]\n",
    "        self.lstm1 = base_model(word_embeds.shape[1], hidden_dim, batch_first = True)\n",
    "        self.lstm2 = base_model(hidden_dim, hidden_dim, batch_first = True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, 3)\n",
    "\n",
    "    def forward(self, text_batch):\n",
    "        DEBUG('######################### FORWARD ############################')\n",
    "        DEBUG(f'text_batch\\n{text_batch}')\n",
    "        \n",
    "        input_lens = seq_lens(text_batch)\n",
    "        \n",
    "        DEBUG(f'input_lens\\n{input_lens}')\n",
    "        sent_vec = torch.from_numpy(embeddings[text_batch]).to(device)\n",
    "        sent_len = len(text_batch[0])\n",
    "        \n",
    "        DEBUG(f'sent_vec\\n{sent_vec.shape}\\n{sent_vec}\\n{sent_vec.device}')\n",
    "        \n",
    "        packed_sent_vec = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            sent_vec.view(-1, sent_len, self.embed_dim),\n",
    "            input_lens,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        ).to(device)\n",
    "        \n",
    "        # ova var se ne koristi, sluzi samo za debug purposes\n",
    "        unpacked_sent_vec, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_sent_vec, batch_first=True)\n",
    "        DEBUG(f'unpacked_sent_vec\\n{unpacked_sent_vec.shape}\\n{unpacked_sent_vec}')\n",
    "        \n",
    "        DEBUG(f'packed_sent_vec\\n{packed_sent_vec}')\n",
    "        \n",
    "        packed_lstm_out, _ = self.lstm1(packed_sent_vec.float())\n",
    "        packed_lstm_out, _ = self.lstm2(packed_lstm_out)\n",
    "        \n",
    "        DEBUG(f'packed_lstm_out\\n{packed_lstm_out}\\n')\n",
    "        \n",
    "        lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_lstm_out, batch_first=True)\n",
    "    \n",
    "        DEBUG(f'lstm_out\\n{lstm_out.shape}\\n{lstm_out}')\n",
    "        \n",
    "        tag_space = self.hidden2tag(lstm_out.view(-1, sent_len, self.hidden_dim))\n",
    "        \n",
    "        DEBUG(f'tag_space\\n{tag_space.shape}\\n{tag_space}')\n",
    "        \n",
    "        DEBUG('######################### END FORWARD ########################')\n",
    "        return tag_space\n",
    "    \n",
    "    def predict(self, numericalized_sent):\n",
    "        with torch.no_grad():\n",
    "            tag_scores = self(numericalized_sent)\n",
    "            labels = TU.labels_from_tensor(tag_scores.view(-1, 3))\n",
    "            return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_length(row):\n",
    "    # row ... instanca objekta Example, i.e. redak u datasetu\n",
    "    _, tokenized = row.text\n",
    "    # row.text u sebi sadrži i raw i tokenized podatke, prvi se odbacuje\n",
    "    return len(tokenized)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "\n",
    "def fit(BSIZE, LR, NEPOCHS, HIDDEN_DIM, base_model, embeddings, vocab): \n",
    "    \n",
    "    trainit = BucketIterator(train, batch_size=BSIZE, bucket_sort_key=instance_length)\n",
    "    model = LSTMTagger(base_model, embeddings, vocab, HIDDEN_DIM).to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    best_score = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    print(f'BSIZE {BSIZE} LR {LR} HIDDEN_DIM {HIDDEN_DIM} NEPOCHS {NEPOCHS}')\n",
    "    prev_score = 0\n",
    "    for epoch in range(NEPOCHS):\n",
    "        for input_batch, target_batch in trainit:\n",
    "            if input_batch.text.shape[0] != BSIZE: continue\n",
    "                \n",
    "            DEBUG('############## FIT ###############')\n",
    "            \n",
    "            text_batch = input_batch.text\n",
    "            label_batch = torch.tensor(target_batch.labels).to(device)\n",
    "            text_batch = torch.tensor(text_batch)\n",
    "            \n",
    "            DEBUG(f'text_batch\\n{text_batch}')\n",
    "            DEBUG(f'label_batch\\n{label_batch}')\n",
    "            \n",
    "            model.zero_grad()            \n",
    "            label_scores = model(text_batch)\n",
    "            \n",
    "            DEBUG(f'label_scores\\n{label_scores.shape}\\n{label_scores}')\n",
    "            DEBUG(f'label_batch\\n{label_batch.shape}\\n{label_batch}')\n",
    "            # radi dobro\n",
    "            loss = loss_function(label_scores.view(BSIZE, 3, -1), label_batch.view(BSIZE, -1))\n",
    "            \n",
    "            DEBUG(f'loss\\n{type(loss)}\\n{loss}\\n{loss.grad_fn}')\n",
    "            VERBOSE(f'loss {loss}')\n",
    "            loss.backward()\n",
    "        \n",
    "            optimizer.step()\n",
    "            if(dbg): return None\n",
    "     \n",
    "        score = 0\n",
    "        failed = 0\n",
    "        succeeded = 0\n",
    "        for row in valid:\n",
    "            spans = row.spans[1]\n",
    "            text = row.text[1]\n",
    "            labels = row.labels[1]\n",
    "            numericalized = [vocab.stoi[word] for word in text]\n",
    "            numericalized = torch.tensor(numericalized)\n",
    "\n",
    "            try:\n",
    "                predicted_labels = model.predict(numericalized.view(1, -1))\n",
    "                predicted_spans = TU.spans_from_labels(row.text[0], predicted_labels, nlp)\n",
    "                score += TU.f1(predicted_spans, spans)\n",
    "                succeeded += 1\n",
    "            except:\n",
    "                failed += 1 # za svaki slucaj\n",
    "\n",
    "        score = float(score) / succeeded\n",
    "        print(f'F1 {score} EPOCH {epoch + 1}')\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save(model, 'best_model.pt')\n",
    "        if(failed != 0): print(f'FAILED {failed}')\n",
    "        prev_score = score\n",
    "        \n",
    "    best_model = torch.load('best_model.pt')        \n",
    "    \n",
    "    return best_model, best_score, f'BSIZE {BSIZE} LR {LR} HIDDEN_DIM {HIDDEN_DIM} ON_EPOCH {best_epoch} FOR_MODEL {base_model}'\n",
    "\n",
    "if dbg: fit(32, 0.001, 1, 512, embeddings, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(batch_sizes, learning_rates, n_epochs, hidden_dims, base_models, embeddings, vocab):\n",
    "    best_model = None\n",
    "    best_score = 0.0\n",
    "    best_desc = ''\n",
    "    for bs in batch_sizes:\n",
    "        for lr in learning_rates:\n",
    "            for n in n_epochs:\n",
    "                for hd in hidden_dims:\n",
    "                    for bms in base_models:\n",
    "                        model, score, model_desc = fit(bs, lr, n, hd, bms, embeddings, vocab)\n",
    "                        print(f'F1 {score} {model_desc}')\n",
    "                        if score > best_score:\n",
    "                            best_model = model\n",
    "                            best_score = score\n",
    "                            best_desc = model_desc\n",
    "    return best_model, best_score, best_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "BSIZE 32 LR 0.001 HIDDEN_DIM 64 NEPOCHS 50\n",
      "F1 0.1754708274438634 EPOCH 1\n",
      "F1 0.24750969405704468 EPOCH 2\n",
      "F1 0.1903381399454217 EPOCH 3\n",
      "F1 0.24750969405704468 BSIZE 32 LR 0.001 HIDDEN_DIM 64 ON_EPOCH 2 FOR_MODEL <class 'torch.nn.modules.rnn.RNN'>\n",
      "BSIZE 32 LR 0.001 HIDDEN_DIM 64 NEPOCHS 50\n",
      "F1 0.11601120945856792 EPOCH 1\n",
      "F1 0.15867521679834448 EPOCH 2\n",
      "F1 0.08074361981767984 EPOCH 3\n",
      "F1 0.15867521679834448 BSIZE 32 LR 0.001 HIDDEN_DIM 64 ON_EPOCH 2 FOR_MODEL <class 'torch.nn.modules.rnn.GRU'>\n",
      "BSIZE 32 LR 0.001 HIDDEN_DIM 64 NEPOCHS 50\n",
      "F1 0.10148794044970955 EPOCH 1\n",
      "F1 0.05014625449758408 EPOCH 2\n",
      "FAILED 41\n",
      "F1 0.10148794044970955 BSIZE 32 LR 0.001 HIDDEN_DIM 64 ON_EPOCH 1 FOR_MODEL <class 'torch.nn.modules.rnn.LSTM'>\n",
      "BSIZE 32 LR 0.001 HIDDEN_DIM 256 NEPOCHS 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "# dobavi najbolji model\n",
    "model, score, desc = grid_search(\n",
    "    batch_sizes = [64], \n",
    "    learning_rates = [0.01], \n",
    "    n_epochs = [50], \n",
    "    hidden_dims = [50], \n",
    "    base_models = [nn.RNN, nn.GRU, nn.LSTM], \n",
    "    embeddings = embeddings, \n",
    "    vocab = vocab)\n",
    "print(f'Best score > {score}')\n",
    "print(f'Achieved for hyperparameters > {desc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/tsd_trial.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "fail=0\n",
    "model.to(device)\n",
    "for idx, row in test.iterrows():\n",
    "    try:\n",
    "        spans = ast.literal_eval(row[0])\n",
    "        text = row[1]\n",
    "\n",
    "        tokenized = tokenizer(text.lower())\n",
    "        _, lemmatized = lemmatizer(text.lower(), tokenized)\n",
    "\n",
    "        numericalized = [vocab.stoi[word] for word in lemmatized]\n",
    "\n",
    "        numericalized = torch.tensor(numericalized)\n",
    "        batch = numericalized.view(1, -1)\n",
    "\n",
    "        predicted_labels = model.predict(batch)\n",
    "\n",
    "        predicted_spans = TU.spans_from_labels(text, predicted_labels, nlp)\n",
    "\n",
    "        row_score = TU.f1(predicted_spans, spans)\n",
    "        score += row_score\n",
    "    except:\n",
    "        fail += 1\n",
    "score /= test.shape[0]\n",
    "print(score)\n",
    "print(fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'bestmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
